# Make Text Generation Predictions Tool

This tool is designed make text generation predictions using a custom model deployed in DataRobot.

For implementation details, please refer to the [custom_model](./custom_model) directory and `custom.py` file.


## How to call the tool
To call or invoke the tool, send a properly formatted JSON request to the deployed modelâ€™s prediction endpoint in DataRobot using its API; the model processes your input and returns a prediction or summary. This can be done programmatically via scripts, command-line tools like `curl`, or integrated into applications.

Please refer to the [making predictions](../README.md#making-predictions) section of documentation for global tools for more technical details on how to call the tool.

### Input structure
When invoking the tool, provide a JSON request as input. The JSON request must include a top-level `payload` object. All parameters listed below should be placed inside this payload object, which will be forwarded to the tool.

**Payload parameters**:
- `input_message` (string): The message or prompt to send to the LLM for generating predictions.
- `deployment_id` (string): The ID of the LLM deployment in DataRobot.

Example:

```json
{
  "payload": {
    "input_message": "Answer my very important question.",
    "deployment_id": "683ee07e7e96db41ab02b263"
  }
}
```

### Output structure
The response will be a JSON object with a key `LLM_Generated_Response` containing the generated text from the LLM, and optional key `Citation_Context` containing the list of sources used for generating the response.

```json
{
  "LLM_Generated_Response": "This is a sample response generated by the LLM.",
  "Citation_Context": [
    {
      "citation_source": "Source 1",
      "citation_output": "This is the text from source 1."
    },
    {
      "citation_source": "Source 2",
      "citation_output": "This is the text from source 2."
    }
  ]
}
```
